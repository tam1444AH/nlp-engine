{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c3ec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pickle(\"reviews_segment.pkl\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad62aa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfefb8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095d9aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20c5acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_review'] = df['review_text'].astype(str).apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba51a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_index = defaultdict(set)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    document_id = row[\"review_id\"].strip(\"'\")\n",
    "    \n",
    "    for word in set(row[\"cleaned_review\"].split()):\n",
    "        word_index[word].add(document_id)\n",
    "\n",
    "print(len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809dad4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_baseline(aspect, opinion, operator1, operator2, operator3, filepath): # This is the baseline boolean function\n",
    "    aspect_tokens = aspect.lower().split()\n",
    "    opinion_tokens = opinion.lower().split()\n",
    "    \n",
    "    a1 = aspect_tokens[0]\n",
    "    a2 = aspect_tokens[1] if len(aspect_tokens) > 1 else None\n",
    "    o1 = opinion_tokens[0] if len(opinion_tokens) > 0 else None\n",
    "    o2 = opinion_tokens[1] if len(opinion_tokens) > 1 else None\n",
    "\n",
    "    a1 = lemmatizer.lemmatize(a1)\n",
    "    a2 = lemmatizer.lemmatize(a2) if a2 else None\n",
    "    o1 = lemmatizer.lemmatize(o1) if o1 else None\n",
    "    o2 = lemmatizer.lemmatize(o2) if o2 else None\n",
    "    \n",
    "    if operator1 == \"AND\":\n",
    "        aspect_docs = word_index.get(a1, set()).intersection(word_index.get(a2, set()))\n",
    "    elif operator1 == \"OR\":\n",
    "        aspect_docs = word_index.get(a1, set()).union(word_index.get(a2, set()))\n",
    "    else:\n",
    "        aspect_docs = word_index.get(a1, set())\n",
    "\n",
    "    if operator2 == \"AND\":\n",
    "        opinion_docs = word_index.get(o1, set()).intersection(word_index.get(o2, set()))\n",
    "    elif operator2 == \"OR\":\n",
    "        opinion_docs = word_index.get(o1, set()).union(word_index.get(o2, set()))\n",
    "    else:\n",
    "        opinion_docs = word_index.get(o1, set())\n",
    "\n",
    "    if operator3 == \"AND\":\n",
    "        result_docs = aspect_docs.intersection(opinion_docs)\n",
    "    elif operator3 == \"OR\":\n",
    "        result_docs = aspect_docs.union(opinion_docs)\n",
    "    else:\n",
    "        result_docs = aspect_docs # We return docs having the aspect if no 3rd operator is given\n",
    "\n",
    "    with open(f\"../Outputs/Baseline/{filepath}\", \"w\") as f:\n",
    "        for document_id in result_docs:\n",
    "            f.write(f\"{document_id}\\n\")\n",
    "        \n",
    "\n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139b6191",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = boolean_baseline(\"audio quality\", \"poor\", operator1=\"AND\", operator2=\"\", operator3=\"\", filepath=\"audio_quality_test1.txt\")\n",
    "results = boolean_baseline(\"audio quality\", \"poor\", operator1=\"AND\", operator2=\"\", operator3=\"AND\", filepath=\"audio_quality_test2.txt\")\n",
    "results = boolean_baseline(\"audio quality\", \"poor\", operator1=\"AND\", operator2=\"\", operator3=\"OR\", filepath=\"audio_quality_test3.txt\")\n",
    "\n",
    "results = boolean_baseline(\"wifi signal\", \"strong\", operator1=\"AND\", operator2=\"\", operator3=\"\", filepath=\"wifi_signal_test1.txt\")\n",
    "results = boolean_baseline(\"wifi signal\", \"strong\", operator1=\"AND\", operator2=\"\", operator3=\"AND\", filepath=\"wifi_signal_test2.txt\")\n",
    "results = boolean_baseline(\"wifi signal\", \"strong\", operator1=\"AND\", operator2=\"\", operator3=\"OR\", filepath=\"wifi_signal_test3.txt\")\n",
    "\n",
    "results = boolean_baseline(\"mouse button\", \"click problem\", operator1=\"AND\", operator2=\"AND\", operator3=\"\", filepath=\"mouse_button_test1.txt\")\n",
    "results = boolean_baseline(\"mouse button\", \"click problem\", operator1=\"AND\", operator2=\"AND\", operator3=\"AND\", filepath=\"mouse_button_test2.txt\")\n",
    "results = boolean_baseline(\"mouse button\", \"click problem\", operator1=\"AND\", operator2=\"AND\", operator3=\"OR\", filepath=\"mouse_button_test3.txt\")\n",
    "\n",
    "results = boolean_baseline(\"gps map\", \"useful\", operator1=\"AND\", operator2=\"\", operator3=\"\", filepath=\"gps_map_test1.txt\")\n",
    "results = boolean_baseline(\"gps map\", \"useful\", operator1=\"AND\", operator2=\"\", operator3=\"AND\", filepath=\"gps_map_test2.txt\")\n",
    "results = boolean_baseline(\"gps map\", \"useful\", operator1=\"AND\", operator2=\"\", operator3=\"OR\", filepath=\"gps_map_test3.txt\")\n",
    "\n",
    "results = boolean_baseline(\"image quality\", \"sharp\", operator1=\"AND\", operator2=\"\", operator3=\"\", filepath=\"image_quality_test1.txt\")\n",
    "results = boolean_baseline(\"image quality\", \"sharp\", operator1=\"AND\", operator2=\"\", operator3=\"AND\", filepath=\"image_quality_test2.txt\")\n",
    "results = boolean_baseline(\"image quality\", \"sharp\", operator1=\"AND\", operator2=\"\", operator3=\"OR\", filepath=\"image_quality_test3.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7454e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_opinion_lexicon(positive_file=\"positive-words.txt\", negative_file=\"negative-words.txt\"):\n",
    "    positive_words = set()\n",
    "    negative_words = set()\n",
    "\n",
    "    with open(positive_file, 'r', encoding='utf-8', errors='ignore') as pos_file:\n",
    "        for line in pos_file:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith(';'):\n",
    "                positive_words.add(line)\n",
    "    \n",
    "    with open(negative_file, 'r', encoding='utf-8', errors='ignore') as neg_file:\n",
    "        for line in neg_file:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith(';'):\n",
    "                negative_words.add(line)\n",
    "    \n",
    "    return positive_words, negative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3027347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words, negative_words = load_opinion_lexicon()\n",
    "\n",
    "print(f\"Length of positive words: {len(positive_words)}\")\n",
    "print(f\"Length of negative words: {len(negative_words)}\")\n",
    "\n",
    "print(\"Sample of positive words:\", list(positive_words)[:10])\n",
    "print(\"Sample of negative words:\", list(negative_words)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e9e48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opinion_sentiment(opinion):\n",
    "\n",
    "    if opinion in positive_words:\n",
    "        return \"positive\"\n",
    "    elif opinion in negative_words:\n",
    "        return \"negative\"\n",
    "    \n",
    "\n",
    "    return \"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555bcd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def m1(aspect, opinion, operator1, operator2, operator3, filepath):\n",
    "    aspect_tokens = aspect.lower().split()\n",
    "    opinion_tokens = opinion.lower().split()\n",
    "\n",
    "    a1 = aspect_tokens[0]\n",
    "    a2 = aspect_tokens[1] if len(aspect_tokens) > 1 else None\n",
    "    o1 = opinion_tokens[0] if len(opinion_tokens) > 0 else None\n",
    "    o2 = opinion_tokens[1] if len(opinion_tokens) > 1 else None\n",
    "\n",
    "    a1 = lemmatizer.lemmatize(a1)\n",
    "    a2 = lemmatizer.lemmatize(a2) if a2 else None\n",
    "    o1 = lemmatizer.lemmatize(o1) if o1 else None\n",
    "    o2 = lemmatizer.lemmatize(o2) if o2 else None\n",
    "\n",
    "    if operator1 == \"AND\":\n",
    "        aspect_docs = word_index.get(a1, set()).intersection(word_index.get(a2, set()))\n",
    "    elif operator1 == \"OR\":\n",
    "        aspect_docs = word_index.get(a1, set()).union(word_index.get(a2, set()))\n",
    "    else:\n",
    "        aspect_docs = word_index.get(a1, set())\n",
    "\n",
    "    if operator2 == \"AND\":\n",
    "        opinion_docs = word_index.get(o1, set()).intersection(word_index.get(o2, set()))\n",
    "    elif operator2 == \"OR\":\n",
    "        opinion_docs = word_index.get(o1, set()).union(word_index.get(o2, set()))\n",
    "    else:\n",
    "        opinion_docs = word_index.get(o1, set())\n",
    "\n",
    "    if operator3 == \"AND\":\n",
    "        result_docs = aspect_docs.intersection(opinion_docs)\n",
    "    elif operator3 == \"OR\":\n",
    "        result_docs = aspect_docs.union(opinion_docs)\n",
    "    else:\n",
    "        result_docs = aspect_docs # We return docs having the aspect if no 3rd operator is given\n",
    "\n",
    "    \n",
    "    sentiment = get_opinion_sentiment(o2) if len(opinion_tokens) > 1 else get_opinion_sentiment(o1)\n",
    "    \n",
    "    print(f\"Sentiment: {sentiment}\")\n",
    "\n",
    "    filtered_results = set()\n",
    "\n",
    "    for document_id in result_docs:\n",
    "        review = df[df['review_id'].str.strip(\"'\") == document_id]\n",
    "        if not review.empty:\n",
    "            rating = review['customer_review_rating']\n",
    "\n",
    "            rating = int(rating.values[0])\n",
    "            if sentiment == \"positive\" and rating > 3:\n",
    "                filtered_results.add(document_id)\n",
    "            elif sentiment == \"negative\" and rating <= 3:\n",
    "                filtered_results.add(document_id)\n",
    "            elif sentiment == \"neutral\":\n",
    "                filtered_results.add(document_id)\n",
    "                \n",
    "    with open(f\"../Outputs/AdvancedModel/{filepath}\", \"w\") as f:\n",
    "        for document_id in filtered_results:\n",
    "            f.write(f\"{document_id}\\n\")\n",
    "\n",
    "    return filtered_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46beb803",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = m1(\"audio quality\", \"poor\", operator1=\"AND\", operator2=\"\", operator3=\"AND\", filepath=\"audio_quality_m1_test4.txt\")\n",
    "\n",
    "results = m1(\"wifi signal\", \"strong\", operator1=\"AND\", operator2=\"\", operator3=\"AND\", filepath=\"wifi_signal_m1_test4.txt\")\n",
    "\n",
    "results = m1(\"mouse button\", \"click problem\", operator1=\"AND\", operator2=\"AND\", operator3=\"AND\", filepath=\"mouse_button_m1_test4.txt\")\n",
    "\n",
    "results = m1(\"gps map\", \"useful\", operator1=\"AND\", operator2=\"\", operator3=\"AND\", filepath=\"gps_map_m1_test4.txt\")\n",
    "\n",
    "results = m1(\"image quality\", \"sharp\", operator1=\"AND\", operator2=\"\", operator3=\"AND\", filepath=\"image_quality_m1_test4.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999a0c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "\n",
    "def m2(aspect, opinion, filepath):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    bert_embeddings = pd.read_pickle(\"data.pkl\")\n",
    "    embeddings = np.stack(bert_embeddings['embedding'].values)\n",
    "    bert_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "    threshold = 0.75\n",
    "    \n",
    "    target_sentence = f\"The {aspect} is {opinion}\"\n",
    "    \n",
    "    query_embeddings = bert_model.encode([target_sentence], convert_to_numpy=True, device=device)\n",
    "\n",
    "    cos_similarities = cosine_similarity(embeddings, query_embeddings).ravel()\n",
    "    matched_indices = np.where(cos_similarities >= threshold)[0]\n",
    "\n",
    "    opinion_tokens = opinion.lower().split()\n",
    "    \n",
    "    o1 = opinion_tokens[0] if len(opinion_tokens) > 0 else None\n",
    "    o2 = opinion_tokens[1] if len(opinion_tokens) > 1 else None\n",
    "\n",
    "    o1 = lemmatizer.lemmatize(o1) if o1 else None\n",
    "    o2 = lemmatizer.lemmatize(o2) if o2 else None\n",
    "\n",
    "    sentiment = get_opinion_sentiment(o2) if len(opinion_tokens) > 1 else get_opinion_sentiment(o1)\n",
    "\n",
    "    result_docs = set()\n",
    "    for index in matched_indices:\n",
    "        document_id = bert_embeddings.iloc[index]['document_id']\n",
    "        \n",
    "        review = df[df['review_id'].str.strip(\"'\") == document_id]\n",
    "        if not review.empty:\n",
    "\n",
    "            rating = review['customer_review_rating']\n",
    "            # We need to convert the rating to a int.\n",
    "            rating = int(rating.values[0])\n",
    "\n",
    "            if sentiment == \"positive\" and rating > 3:\n",
    "                result_docs.add(document_id)\n",
    "            elif sentiment == \"negative\" and rating <= 3:\n",
    "                result_docs.add(document_id)\n",
    "            elif sentiment == \"neutral\": # If the opinion is actually neutral OR the user did not provide an opinion\n",
    "                result_docs.add(document_id)\n",
    "                \n",
    "    with open(f\"../Outputs/AdvancedModel/{filepath}\", \"w\") as f:\n",
    "        for document_id in result_docs:\n",
    "            f.write(f\"{document_id}\\n\")\n",
    "\n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f04d2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = m2(\"audio quality\", \"poor\", filepath=\"audio_quality_m2_test4.txt\")\n",
    "\n",
    "results = m2(\"wifi signal\", \"strong\", filepath=\"wifi_signal_m2_test4.txt\")\n",
    "\n",
    "results = m2(\"mouse button\", \"click problem\", filepath=\"mouse_button_m2_test4.txt\")\n",
    "\n",
    "results = m2(\"gps map\", \"useful\", filepath=\"gps_map_m2_test4.txt\")\n",
    "\n",
    "results = m2(\"image quality\", \"sharp\", filepath=\"image_quality_m2_test4.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
